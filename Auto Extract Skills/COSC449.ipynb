{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Skills Honour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scapping Monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.monster.com/jobs/search/?where=Canada\"\n",
    "page = requests.get(URL)\n",
    "print(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "results = soup.find(id=\"ResultsContainer\")\n",
    "\n",
    "# Look for jobs\n",
    "python_jobs = results.find_all(\"h2\", string=lambda t: \"python\" in t.lower())\n",
    "for p_job in python_jobs:\n",
    "    link = p_job.find(\"a\")[\"href\"]\n",
    "    print(p_job.text.strip())\n",
    "    print(f\"Apply here: {link}\\n\")\n",
    "\n",
    "# Print out all available jobs from the scraped webpage\n",
    "job_elems = results.find_all(\"section\", class_=\"card-content\")\n",
    "for job_elem in job_elems:\n",
    "    title_elem = job_elem.find(\"h2\", class_=\"title\")\n",
    "    company_elem = job_elem.find(\"div\", class_=\"company\")\n",
    "    location_elem = job_elem.find(\"div\", class_=\"location\")\n",
    "    if None in (title_elem, company_elem, location_elem):\n",
    "        continue\n",
    "    print(title_elem.text.strip())\n",
    "    print(company_elem.text.strip())\n",
    "    print(location_elem.text.strip())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping From Indeed with Description of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "cities = []\n",
    "with open(\"Cities.csv\", \"r\",encoding = \"ISO-8859-1\") as infile:\n",
    "    csvfile = csv.reader(infile)\n",
    "    for row in csvfile:\n",
    "        cities.append(row[0])\n",
    "#print (cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/jobs?q=junior developer&l=Delson&radius=0\n",
      "1\n",
      "Delson\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Desbiens&radius=0\n",
      "1\n",
      "Desbiens\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Deux-Montagnes&radius=0\n",
      "1\n",
      "Deux-Montagnes\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Disraeli&radius=0\n",
      "1\n",
      "Disraeli\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Dolbeau-Mistassini&radius=0\n",
      "1\n",
      "Dolbeau-Mistassini\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Dollard-des-Ormeaux&radius=0\n",
      "1\n",
      "Dollard-des-Ormeaux\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Donnacona&radius=0\n",
      "1\n",
      "Donnacona\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Dorval&radius=0\n",
      "1\n",
      "Dorval\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Drummondville&radius=0\n",
      "1\n",
      "Drummondville\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Dunham&radius=0\n",
      "1\n",
      "Dunham\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Duparquet&radius=0\n",
      "1\n",
      "Duparquet\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=East Angus&radius=0\n",
      "1\n",
      "East Angus\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Estérel&radius=0\n",
      "0\n",
      "Estérel\n",
      "0\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Farnham&radius=0\n",
      "1\n",
      "Farnham\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Fermont&radius=0\n",
      "1\n",
      "Fermont\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Forestville&radius=0\n",
      "1\n",
      "Forestville\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Fossambault-sur-le-Lac&radius=0\n",
      "0\n",
      "Fossambault-sur-le-Lac\n",
      "0\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Gaspé&radius=0\n",
      "1\n",
      "Gaspé\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Gatineau&radius=0\n",
      "1\n",
      "Gatineau\n",
      "1\n",
      "https://ca.indeed.com/jobs?q=junior developer&l=Gracefield&radius=0\n",
      "1\n",
      "Gracefield\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "from langdetect import detect\n",
    "\n",
    "#url to scrape\n",
    "def get_url(position,location):\n",
    "    template = 'https://ca.indeed.com/jobs?q={}&l={}&radius=0'\n",
    "    \n",
    "    url = template.format(position,location)\n",
    "    return url\n",
    "\n",
    "def get_record(card):\n",
    "    atag = card.h2.a\n",
    "    job_title = atag.get('title')\n",
    "    job_url = 'https://ca.indeed.com' + atag.get('href')\n",
    "    \n",
    "    page = requests.get(job_url)\n",
    "    soup2 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    if type(soup2.find(id='jobDescriptionText'))==type(None):\n",
    "        description=\"\"\n",
    "    else:\n",
    "        description = soup2.find(id='jobDescriptionText').text.strip()\n",
    "        \n",
    "    #print(detect(description))\n",
    "    \n",
    "    if type(card.find('span', 'company'))==type(None):\n",
    "        company=\"\"\n",
    "    else:\n",
    "        company = card.find('span', 'company').text.strip()\n",
    "        \n",
    "    \n",
    "    if type(card.find('span', 'remote'))==type(None):\n",
    "        isRemote = False\n",
    "    else:\n",
    "        isRemote = True\n",
    "        \n",
    "    job_summary = card.find('div', 'summary').text.strip()\n",
    "    post_date = card.find('span', 'date').text\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        job_salary = card.find('span','salaryText').text.strip()\n",
    "    except  AttributeError:\n",
    "        job_salary = ''\n",
    "    \n",
    "    record = (job_title, company, isRemote, description)\n",
    "    #record = (job_title)\n",
    "    \n",
    "    return record\n",
    "\n",
    "#extract the job data\n",
    "def main(position, location):\n",
    "    records = []\n",
    "    url = get_url(position, location)\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        print(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find_all('div', 'jobsearch-SerpJobCard')\n",
    "        print(len(cards))\n",
    "        \n",
    "        print(location)\n",
    "    \n",
    "        for card in cards:\n",
    "            if type(card.find('span', 'location accessible-contrast-color-location'))!=type(None):\n",
    "                job_location = card.find('span', 'location accessible-contrast-color-location').text\n",
    "            elif type(card.find('div', 'location accessible-contrast-color-location'))!=type(None):\n",
    "                job_location = card.find('div', 'location accessible-contrast-color-location').text\n",
    "            else:\n",
    "                job_location=''\n",
    "            temp_Location = job_location.split(',')[0].lower()\n",
    "            \n",
    "            #print(job_location)\n",
    "            \n",
    "            if(temp_Location==location.lower() or temp_Location=='canada'):\n",
    "                record = get_record(card)\n",
    "                if record[3]!=\"\":\n",
    "                    #print(record)\n",
    "                    record = record + (job_location,)\n",
    "                    records.append(record)\n",
    "        \n",
    "        try:\n",
    "            url = 'https://ca.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    print(len(records))\n",
    "    \n",
    "    if(len(records) != 0):\n",
    "        if(len(location) > 30):\n",
    "            location = location[0:29]\n",
    "        worksheet = workbook.create_sheet(title = location)\n",
    "        count = 1;\n",
    "    \n",
    "        #element[job_title, company, isRemote, description, job_Location]\n",
    "        for element in records:\n",
    "            worksheet['A' + str(count)] = element[0]\n",
    "            worksheet['B' + str(count)] = element[4]\n",
    "            worksheet['C' + str(count)] = element[1]\n",
    "            worksheet['D' + str(count)] = element[2]\n",
    "            worksheet['E' + str(count)] = element[3]\n",
    "            count += 1;\n",
    "        workbook.save(filename = key + '.xlsx')\n",
    "\n",
    "key = 'junior developer'\n",
    "workbook = load_workbook(filename = key + '.xlsx')\n",
    "x = 217\n",
    "for x in range(x, x + 20):\n",
    "    if(x < len(cities)):\n",
    "        main(key, cities[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook(filename = 'empty_book.xlsx')\n",
    "\n",
    "ws2 = wb.create_sheet(title=\"P\")\n",
    "ws2['F5'] = 3.14\n",
    "\n",
    "wb.save(filename = 'empty_book.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"giant_panda\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"\"\"Work with Hadoop and Big Data stack on building\n",
    "data pipelines for streaming and batch processing\n",
    "of the data using Lambda architecture.\n",
    "Product expert for Hadoop and Big Data\n",
    "- including Hive, KNOX and Sqoop.\"\"\")\n",
    "text1 = (\"Mac OS\")\n",
    "doc = nlp(text1)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "nounList = [chunk.text for chunk in doc.noun_chunks]\n",
    "verbList = [token.lemma_ for token in doc if token.pos_ == \"VERB\"] \n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"join our team\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Skills from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "n = 0\n",
    "skills = []\n",
    "other = []\n",
    "\n",
    "with open(\"skills_2.csv\", \"r\") as infile:\n",
    "    csvfile = csv.reader(infile)\n",
    "    for row in csvfile: \n",
    "        n += 1\n",
    "        #print(row[0])\n",
    "        if(len(row) == 1):\n",
    "            doc = nlp(row[0].lower())\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Analyze syntax\n",
    "        nounList = [chunk.text for chunk in doc.noun_chunks]\n",
    "        verbList = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "        #print(len(verbList), len(nounList))\n",
    "        \n",
    "        #find all the skills with only one noun phrase\n",
    "        if(len(verbList) == 0 and len(nounList) == 1):\n",
    "            skills.append(row[0])\n",
    "        else:\n",
    "            other.append(row[0])\n",
    "            \n",
    "infile.close()\n",
    "\n",
    "print(n)\n",
    "print(len(skills))\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"output.csv\", \"w\")\n",
    "for line in skills:\n",
    "    outfile.write(line + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"others.csv\", \"w\")\n",
    "for line in other:\n",
    "    outfile.write(line + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary\n",
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "POSTGRES_ADDRESS ='localhost'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_USERNAME = 'postgres'\n",
    "POSTGRES_PASSWORD ='q68e3q'\n",
    "POSTGRES_DBNAME ='JobSkills_db'\n",
    "\n",
    "conn=psycopg2.connect(user=POSTGRES_USERNAME,\n",
    "                                password=POSTGRES_PASSWORD,\n",
    "                                host=POSTGRES_ADDRESS,\n",
    "                                port=POSTGRES_PORT,\n",
    "                                database=POSTGRES_DBNAME)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "postgres_str=('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'\n",
    "                        .format(username=POSTGRES_USERNAME,\n",
    "                                password=POSTGRES_PASSWORD,\n",
    "                                ipaddress=POSTGRES_ADDRESS,\n",
    "                                port=POSTGRES_PORT,\n",
    "                                dbname=POSTGRES_DBNAME))\n",
    "cnx=create_engine(postgres_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create table for location\n",
    "cur.execute(\n",
    "    '''Create table location(\n",
    "    lid int,\n",
    "    lname varchar,\n",
    "    Primary Key (lid));''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table for jobtitle\n",
    "cur.execute(\n",
    "    '''Create table jobtitle(\n",
    "    jid int,\n",
    "    jname varchar,\n",
    "    industry varchar,\n",
    "    Primary Key (jid));''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table for jobtpostings\n",
    "cur.execute(\n",
    "    '''create table jobpostings(\n",
    "    pid int,\n",
    "    title varchar,\n",
    "    lid int,\n",
    "    company varchar,\n",
    "    isRemote boolean,\n",
    "    description varchar,\n",
    "    jid int,\n",
    "    PRIMARY KEY (pid),\n",
    "    FOREIGN KEY (lid) REFERENCES location(lid),\n",
    "    FOREIGN KEY (jid) REFERENCES jobtitle(jid))''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#insert cities into location\n",
    "import csv\n",
    "\n",
    "i = 1\n",
    "with open(\"Cities.csv\", \"r\",encoding = \"ISO-8859-1\") as infile:\n",
    "    csvfile = csv.reader(infile)\n",
    "    for row in csvfile:\n",
    "        sql=\"Insert INTO location(lid, lname) VALUES(%s,%s)\"\n",
    "        cur.execute(sql,(str(i),row[0]))\n",
    "        i += 1\n",
    "cur.execute('''Insert INTO location(lid, lname) VALUES(0,'Remote')''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert job titles\n",
    "import csv\n",
    "\n",
    "i = 1\n",
    "with open(\"Job_titles.csv\", \"r\") as infile:\n",
    "    csvfile = csv.reader(infile)\n",
    "    for row in csvfile:\n",
    "        sql=\"Insert INTO jobtitle(jid, jname, industry) VALUES(%s,%s,%s)\"\n",
    "        cur.execute(sql,(str(i),row[0],row[1]))\n",
    "        i += 1\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('''select * from jobtitle;''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the i\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from langdetect import detect\n",
    "\n",
    "#need to change the file name for insert\n",
    "jobtitle = \"farmer\"\n",
    "workbook = load_workbook(filename = 'zdata/'+ jobtitle +'.xlsx')\n",
    "\n",
    "cur2 = conn.cursor()\n",
    "sql2=\"select jid from jobtitle where jname = '\" + jobtitle + \"'\"\n",
    "cur2.execute(sql2)\n",
    "records = cur2.fetchall()\n",
    "jid = records[0][0]\n",
    "\n",
    "for worksheet in workbook:\n",
    "    for row in worksheet.values:\n",
    "        if (row[1].lower()==\"canada\" and row[3]==True):\n",
    "            location=\"Remote\" \n",
    "        else: \n",
    "            location=worksheet.title\n",
    "        sql2=\"select lid from location where lname like %s\"\n",
    "        cur2.execute(sql2,[location + \"%\"])\n",
    "        records = cur2.fetchall()\n",
    "        lid = records[0][0]\n",
    "        \n",
    "        #insert another item if it is a remote job with a specific location\n",
    "        if(lid != 0 and row[3] == True):\n",
    "            sql2=\"Insert INTO jobpostings(pid, title, lid, company, isRemote, description, jid) VALUES(%s,%s,%s,%s,%s,%s,%s)\"\n",
    "            cur2.execute(sql2,(i,row[0],0,row[2],row[3],row[4],jid))\n",
    "            i += 1\n",
    "        \n",
    "        #ignore non-English descriptions\n",
    "        if detect(row[4]) == 'en':\n",
    "            sql=\"Insert INTO jobpostings(pid, title, lid, company, isRemote, description, jid) VALUES(%s,%s,%s,%s,%s,%s,%s)\"\n",
    "            cur.execute(sql,(i,row[0],lid,row[2],row[3],row[4],jid))\n",
    "            i += 1\n",
    "print(i)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate postings from jobpostings\n",
    "cur.execute(\"select * from jobpostings\")\n",
    "records = cur.fetchall()\n",
    "new = []\n",
    "\n",
    "for element in records:\n",
    "    repeated = False \n",
    "    for ele in new:\n",
    "        if element[1] == ele[1] and element[3] == ele[3] and element[2]==0:\n",
    "            repeated = True\n",
    "    if repeated == False:\n",
    "        new.append(element)\n",
    "        #print(element[0])\n",
    "        \n",
    "for ele in new:\n",
    "    sql = \"Delete from jobpostings where title = %s and company = %s\"\n",
    "    cur.execute(sql, (ele[1], ele[3]))\n",
    "    sql=\"Insert INTO jobpostings(pid, title, lid, company, isRemote, description, jid) VALUES(%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    cur.execute(sql, (ele[0], ele[1], ele[2], ele[3], ele[4], ele[5], ele[6]))\n",
    "    \n",
    "print(len(new))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_sql_query('''select * from jobpostings;''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''Delete from jobpostings''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "POSTGRES_ADDRESS ='localhost'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_USERNAME = 'postgres'\n",
    "POSTGRES_PASSWORD ='q68e3q'\n",
    "POSTGRES_DBNAME ='JobSkills_db'\n",
    "\n",
    "conn=psycopg2.connect(user=POSTGRES_USERNAME,\n",
    "                                password=POSTGRES_PASSWORD,\n",
    "                                host=POSTGRES_ADDRESS,\n",
    "                                port=POSTGRES_PORT,\n",
    "                                database=POSTGRES_DBNAME)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "postgres_str=('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'\n",
    "                        .format(username=POSTGRES_USERNAME,\n",
    "                                password=POSTGRES_PASSWORD,\n",
    "                                ipaddress=POSTGRES_ADDRESS,\n",
    "                                port=POSTGRES_PORT,\n",
    "                                dbname=POSTGRES_DBNAME))\n",
    "cnx=create_engine(postgres_str)\n",
    "\n",
    "#save all the descriptions for data analyst in des[]\n",
    "cur.execute(\"select * from jobtitlepostings where jid = 1\")\n",
    "records = cur.fetchall()\n",
    "des = []\n",
    "for element in records:\n",
    "    des.append(element[5])\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  \n",
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"alice.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "  \n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) \n",
    "  \n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "          \"and 'wonderland' - Skip Gram : \", \n",
    "    model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "            \"and 'machines' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.42B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are first going to be using a method known as t-distributed stochastic neighbor embedding, also known as t-SNE. t-SNE will allow us to reduce the, in my case, 50 dimensions of the data, down to 2 dimensions.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "n_components specifies the number of dimensions to reduce the data into.\n",
    "\n",
    "random_state is a seed we can use to obtain consistent results.\n",
    "\n",
    "After initializing the t-SNE class, we need to get a list of every word, and the corresponding vector to that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "#taking all the keys of embeddings_dict and converts it to a list.\n",
    "words =  list(embeddings_dict.keys())\n",
    "#uses list comprehension to obtain the value in embeddings_dict that corresponds to each word we chose, \n",
    "#and put that into list form.\n",
    "vectors = [embeddings_dict[word] for word in words]\n",
    "\n",
    "Y = tsne.fit_transform(vectors[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "workbook = load_workbook(filename = 'c.xlsx')\n",
    "worksheet = workbook['raw']\n",
    "#print(worksheet['B1'].value)\n",
    "#worksheet['C1'] = 1111\n",
    "count = 1\n",
    "invalid = []\n",
    "vectors = []\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "while(worksheet['A' + str(count)].value != None):\n",
    "    temp = worksheet['A' + str(count)].value.split(\" \")\n",
    "    count += 1\n",
    "    ele = []\n",
    "    vector = 0\n",
    "    try:\n",
    "        for e in temp:\n",
    "            vector += embeddings_dict[e.lower()]\n",
    "    except:\n",
    "        invalid.append(count)\n",
    "        continue\n",
    "    vectors.append(vector)\n",
    "\n",
    "x = tsne.fit_transform(vectors)\n",
    "\n",
    "k = 0\n",
    "for i in range(count - 1):\n",
    "    if(len(invalid) > k):\n",
    "        if(i + 1 == invalid[k]):\n",
    "            k += 1\n",
    "            i += 1\n",
    "            continue\n",
    "    print(x[:, 0][i - k], x[:, 1][i - k])\n",
    "    worksheet['C' + str(i + 1)] = x[:, 0][i - k]\n",
    "    worksheet['D' + str(i + 1)] = x[:, 1][i - k]\n",
    "    i += 1\n",
    "    \n",
    "print(x[:, 0][len(x[:,0]) - 1], x[:, 1][len(x[:,0]) - 1])\n",
    "workbook.save(filename = 'c.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
